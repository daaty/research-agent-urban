# Project: Research Agent

**Objetivo:**
Este projeto automatiza a extra√ß√£o e sumariza√ß√£o de conte√∫do de company websites e LinkedIn profiles usando um LLM local (Mistral via Ollama) e t√©cnicas de web scraping. Agora tamb√©m inclui funcionalidade para scraping de p√°ginas autenticadas com extra√ß√£o de dados de tabelas.

## üéØ Funcionalidades Principais

### 1. Scraping Original (LinkedIn + Websites)
* Scrapes designated company websites e LinkedIn profiles (requires authenticated session for LinkedIn).
* Utilizes a local Mistral LLM instance via Ollama for content summarization.
* Employs carefully engineered prompts to generate structured summaries.
* Operates without reliance on external LLM APIs.

### 2. üÜï Scraping de P√°ginas Autenticadas (Rides Dashboard)
* ‚úÖ **Login automatizado** em p√°ginas que exigem autentica√ß√£o
* ‚úÖ **Extra√ß√£o de dados de m√∫ltiplas tabelas** (at√© 5 tabelas diferentes)
* ‚úÖ **Integra√ß√£o com n8n** para envio dos dados extra√≠dos
* ‚úÖ **Detec√ß√£o e tratamento de tabelas vazias**
* ‚úÖ **Captura de tr√°fego de rede** para debug

## üöÄ Status do Projeto

### ‚úÖ CONCLU√çDO - Fase 1: Autentica√ß√£o
- Login automatizado funcionando
- Acesso ao dashboard autenticado
- Navega√ß√£o p√≥s-login confirmada

### üîÑ EM ANDAMENTO - Fase 2: Extra√ß√£o de Dados
- Identifica√ß√£o das tabelas no dashboard
- Implementa√ß√£o da extra√ß√£o de dados
- Estrutura√ß√£o dos dados extra√≠dos

### üìã PLANEJADO - Fase 3: Integra√ß√£o
- Envio dos dados para n8n
- Resumo dos dados com LLM local
- Monitoramento e logs

## üõ†Ô∏è **Technology Stack:**
* **Web Scraping:** Playwright / Puppeteer
* **LLM Engine:** Ollama (running Mistral model locally)
* **Language:** TypeScript (following Clean Architecture principles)
* **Authentication:** Automated login with session management

## üìö Documenta√ß√£o Detalhada

- [`PROGRESS.md`](PROGRESS.md) - Progresso detalhado e li√ß√µes aprendidas
- [`USAGE_EXAMPLES.md`](USAGE_EXAMPLES.md) - Exemplos de uso e troubleshooting

## üéØ Endpoints Dispon√≠veis

### Scraping Original
- `POST /api/scrape` - Scraping original com LinkedIn
- `POST /api/scrape-tables` - Scraping gen√©rico de tabelas

### Rides Dashboard (Autenticado)
- `POST /api/rides/test-enter-login` - ‚úÖ Login funcional
- `POST /api/rides/test-simple` - Teste de acesso b√°sico
- `POST /api/rides/investigate-form` - Investiga√ß√£o de formul√°rios
- `POST /api/rides/capture-network` - Captura de tr√°fego de rede
- `POST /api/rides/scrape-full` - üîÑ Scraping completo (em desenvolvimento)

## **Example Output (Loopio):**

* **Website Summary:** Loopio is presented as an RFP automation platform simplifying response management with features like a secure content library and AI assistance. It emphasizes collaboration, consistency, integration capabilities (CRM, Slack, etc.), flexible pricing, and dedicated support. Its AI is user-controlled, focusing on data-driven insights, human review, and quality control.
* **LinkedIn Summary (Profile: Matt York):** Key topics identified include Machine Learning opportunities at Loopio and Employee Appreciation Day reflections. Updates mention hiring for a Senior ML Engineer (Remote, Toronto-based) and celebrating team growth despite challenges. No formal blog posts were detected; content resembled internal updates. No major company announcements beyond hiring and cultural posts were noted.

## **Usage:**
1.  Ensure the Ollama server is running with the Mistral model available (`ollama run mistral`).
2.  Start the Node.js backend server (e.g., `npm run dev`).
3.  Send a POST request to the desired endpoint:

### Scraping Original:
```bash
curl -X POST http://localhost:3000/api/scrape \
-H "Content-Type: application/json" \
-d '{"companyUrl":"https://loopio.com/", "linkedinUrl":"https://www.linkedin.com/in/myork/"}'
```

### Rides Dashboard Login:
```bash
curl -X POST https://sturdy-space-spoon-7g5gx9w5r6whrpvv-3000.app.github.dev/api/rides/test-enter-login \
-H "Content-Type: application/json" \
-d '{}'
```

4.  The API will return a JSON response containing the generated summaries or extracted data.

## üîß Setup

```bash
# Install dependencies
npm install

# Install Playwright browsers
npx playwright install

# Configure environment variables
cp .env.example .env
# Edit .env with your credentials

# Start development server
npm run dev
```
